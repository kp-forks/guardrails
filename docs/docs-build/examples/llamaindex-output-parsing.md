import CodeOutputBlock from '../../docusaurus/code-output-block.jsx';

<a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/output_parsing/GuardrailsDemo.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->

# Guardrails Output Parsing (Deprecated)

## DEPRECATION NOTE
This integration between LlamaIndex and Guardrails is only valid for llama-index ~0.9.x and guardrails-ai < 0.5.x. and thus has been deprecated.  For an updated example of using Guardrails with LlamaIndex with their latest versions, see the [GuardrailsEngine](/docs/integrations/llama_index)


If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.

#### Download Data


```bash
mkdir -p 'data/paul_graham/'
wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'
```

<CodeOutputBlock lang="bash">

```
    --2023-12-01 10:30:22--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt
    Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'
    Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...
    Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 75042 (73K) [text/plain]
    Saving to: â€˜data/paul_graham/paul_graham_essay.txtâ€™
    
    data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.02s   
    
    2023-12-01 10:30:23 (3.82 MB/s) - â€˜data/paul_graham/paul_graham_essay.txtâ€™ saved [75042/75042]
    
```

</CodeOutputBlock>

#### Load documents, build the VectorStoreIndex



```python
import logging
import sys
from llama_index import VectorStoreIndex, SimpleDirectoryReader

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))
```


```python
# load documents
documents = SimpleDirectoryReader("./data/paul_graham/").load_data()
```


```python
index = VectorStoreIndex.from_documents(documents, chunk_size=512)
```

<CodeOutputBlock lang="python">

```
    INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
    HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
    INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
    HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
    INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
    HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
```

</CodeOutputBlock>

#### Define Query + Guardrails Spec



```python
from llama_index.output_parsers import GuardrailsOutputParser
from llama_index.llm_predictor import StructuredLLMPredictor
```


```python
llm_predictor = StructuredLLMPredictor()
```

**Define custom QA and Refine Prompts**



```python
from llama_index.prompts import PromptTemplate
from llama_index.prompts.default_prompts import (
    DEFAULT_TEXT_QA_PROMPT_TMPL,
    DEFAULT_REFINE_PROMPT_TMPL,
)
```

**Define Guardrails Spec**



```python
# You can either define a RailSpec and initialise a Guard object for_rail_string()
# OR define Pydantic classes and initialise a Guard object for_pydantic()
# For more info: https://docs.guardrailsai.com/defining_guards/pydantic/
# Guardrails recommends Pydantic

from pydantic import BaseModel, Field
import guardrails as gd


class Point(BaseModel):
    # In all the fields below, you can define validators as well
    # Left out for brevity
    explanation: str = Field(description="The first thing the author worked on")
    explanation2: str = Field(description="The second thing the author worked on")
    explanation3: str = Field(description="The third thing the author worked on")


class BulletPoints(BaseModel):
    points: Point = Field(
        description="Bullet points regarding events in the author's life."
    )


# Define the prompt
prompt = """
Query string here.

${output_schema}

${gr.json_suffix_prompt_v2_wo_none}
"""
```


```python
# Create a guard object
guard = gd.Guard.for_pydantic(output_class=BulletPoints, prompt=prompt)

# Create output parse object
output_parser = GuardrailsOutputParser(guard, llm=llm_predictor.llm.complete)
```


```python
# NOTE: we use the same output parser for both prompts, though you can choose to use different parsers
# NOTE: here we add formatting instructions to the prompts.

fmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)
fmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)

qa_prompt = PromptTemplate(fmt_qa_tmpl, output_parser=output_parser)
refine_prompt = PromptTemplate(fmt_refine_tmpl, output_parser=output_parser)
```


```python
# take a look at the new QA template!
print(fmt_qa_tmpl)
```

<CodeOutputBlock lang="python">

```
    Context information is below.
    ---------------------
    {context_str}
    ---------------------
    Given the context information and not prior knowledge, answer the query.
    Query: {query_str}
    Answer: 
    
    ct name="points" description="Bullet points regarding events in the author's life.">
            <string name="explanation" description="The first thing the author worked on"/>
            <string name="explanation2" description="The second thing the author worked on"/>
            <string name="explanation3" description="The third thing the author worked on"/>
        </object>
    </output>
    
    
    
    ONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.
    
    
```

</CodeOutputBlock>

#### Query Index



```python
query_engine = index.as_query_engine(
    text_qa_template=qa_prompt,
    refine_template=refine_prompt,
    llm_predictor=llm_predictor,
)
response = query_engine.query(
    "What are the three items the author did growing up?",
)
```

<CodeOutputBlock lang="python">

```
    INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
    HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
    INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
    HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
```

</CodeOutputBlock>


```python
print(response)
```

<CodeOutputBlock lang="python">

```
    {
      "points": {
        "explanation": "writing",
        "explanation2": "programming",
        "explanation3": "philosophy"
      }
    }
```

</CodeOutputBlock>
