import CodeOutputBlock from '../../../docusaurus/code-output-block.jsx';

```bash
guardrails hub install hub://guardrails/toxic_language --quiet
```

<CodeOutputBlock lang="bash">

```
    Installing hub://guardrails/toxic_language...
    âœ…Successfully installed guardrails/toxic_language!
    
    
```

</CodeOutputBlock>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->

## Check whether an LLM-generated response contains toxic language

### Using the `ToxicLanguage` validator

This is a simple walkthrough of the `ToxicLanguage` validator. This validator checks whether an LLM-generated response contains toxic language. It uses the pre-trained multi-label model from HuggingFace -`unitary/unbiased-toxic-roberta` to check whether the generated text is toxic. It supports both full-text-level and sentence-level validation.


```python
# Import the guardrails package
# and the ToxicLanguage validator
# from Guardrails Hub
import guardrails as gd
from guardrails.hub import ToxicLanguage
from rich import print
```

<CodeOutputBlock lang="python">

```
    /Users/dtam/.pyenv/versions/3.12.3/envs/litellm/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)
      from tqdm.autonotebook import tqdm, trange
```

</CodeOutputBlock>


```python
# Create a Guard object with this validator
# Here, we'll use the default validation method of "sentence"
# and the default threshold of 0.25

guard = gd.Guard().use(ToxicLanguage(on_fail="fix"))
```


```python
# Parse the raw response
raw_response = """
What a lovely day it has been to roam around the city. I love it here!
I hate how pathetic and needy you can be sometimes.
"""
raw_llm_output, validated_output, *rest = guard.parse(
    llm_output=raw_response,
)

# Print the output
print(validated_output)
```
    
<CodeOutputBlock dangerouslySetInnerHTML={{ __html: "/Users/dtam/dev/guardrails/guardrails/validator_service/__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.<br />      warnings.warn(<br /><br /><br /><br /><br /><pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><br />What a lovely day it has been to roam around the city.<br />I love it here!<br /></pre>"}} />

As you can see here, the sentence `I hate how pathetic and needy you can be sometimes.` has been detected as toxic, and hence removed from the validated output.



```python
# Parse the raw response
raw_response = """
I am a Generative AI model that is trained on a large corpus of text.
I am shocked by how disgusting and vile you are.
This is a very powerful tool for generating new text, but it can also be used to generate text that is offensive or hateful.
"""
raw_llm_output, validated_output, *rest = guard.parse(
    llm_output=raw_response,
)

# Print the output
print(validated_output)
```
    
<CodeOutputBlock dangerouslySetInnerHTML={{ __html: "/Users/dtam/dev/guardrails/guardrails/validator_service/__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.<br />      warnings.warn(<br /><br /><br /><br /><br /><pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><br />I am a Generative AI model that is trained on a large corpus of text.<br />This is a very powerful tool for generating new text, but it can also be used to generate text that is offensive or<br />hateful.<br /></pre>"}} />

Similarly, here the sentence `I am shocked by how disgusting and vile you are.` has been detected as toxic, and hence removed from the validated output.



```python
# Test with validation method 'full'
full_guard = gd.Guard().use(ToxicLanguage(validation_method="full", on_fail="fix"))
```


```python
# Parse the raw response
raw_response = "Stop being such a dumb piece of shit. Why can't you comprehend this?"
raw_llm_output, validated_output, *rest = full_guard.parse(
    llm_output=raw_response,
)

# Print the output
print(validated_output)
```
    
<CodeOutputBlock dangerouslySetInnerHTML={{ __html: "/Users/dtam/dev/guardrails/guardrails/validator_service/__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.<br />      warnings.warn(<br /><br /><br /><br /><br /><pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Why can't you comprehend this?<br /></pre>"}} />

Here, we're doing validation on the entire text, and toxic language was detected here - hence, the nothing is returned here.

